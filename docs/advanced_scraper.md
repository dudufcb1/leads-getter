# Advanced Scraper Documentation

## Overview

This document describes the advanced scraping features **COMPLETED** ‚úÖ in Phase 2 of the leads generator system. The advanced scraper provides robust, production-ready web scraping capabilities with comprehensive monitoring, error handling, and data quality controls.

## üéØ **PHASE 2 STATUS: FULLY IMPLEMENTED AND TESTED**

All planned features have been successfully implemented and all integration tests pass:

- ‚úÖ **Database Models**: Advanced schema with quality scoring and metadata
- ‚úÖ **Scraper Engine**: All pipelines and middlewares working correctly
- ‚úÖ **API Integration**: Complete REST API with job management
- ‚úÖ **Integration Tests**: All tests passing (3/3 successful)

### Quick Stats
- **10 Major Features** implemented
- **50+ Configuration Settings** available
- **4 Advanced Pipelines** for data processing
- **15+ Email Detection Patterns** for comprehensive extraction
- **100% Test Success Rate** in integration testing

## Key Features

### 1. Chained Scraping with Depth Management
- **Multi-page crawling**: Automatically follows links to discover new pages
- **Depth control**: Configurable maximum depth to prevent infinite crawling
- **Smart URL filtering**: Avoids duplicate URLs and respects robots.txt
- **Domain-aware crawling**: Tracks crawling per domain with separate depth limits

### 2. Advanced Email Detection
- **Multiple regex patterns**: Catches various email formats including obfuscated emails
- **Format validation**: Ensures extracted emails follow proper email standards
- **Quality scoring**: Rates email quality based on format, domain, and context
- **Duplicate prevention**: Intelligent deduplication across pages and domains

### 3. Courtesy Parameters and Rate Limiting
- **Adaptive delays**: Dynamic delays based on domain request frequency
- **User-agent rotation**: Multiple user-agents to avoid detection
- **Rate limiting**: Configurable requests per minute per domain
- **Respectful crawling**: Built-in delays to avoid overwhelming servers

### 4. Robust Error Handling
- **Retry mechanisms**: Exponential backoff for failed requests
- **Timeout management**: Configurable timeouts for different operations
- **Block detection**: Automatic detection and handling of anti-bot measures
- **Request fingerprinting**: Tracks and avoids problematic request patterns

### 5. Advanced Filtering Pipelines
- **Language detection**: Filters content by detected language
- **Duplicate detection**: Prevents processing of duplicate content
- **Quality filtering**: Scores and filters content based on various criteria
- **Spam detection**: Identifies and filters out spam content

### 6. Comprehensive Monitoring
- **Database logging**: All scraping activities logged to database
- **Session tracking**: Complete session management with statistics
- **Performance metrics**: Real-time monitoring of scraping performance
- **Error reporting**: Detailed error logging with context

## Configuration

### Scrapy Settings (`backend/app/scraper/settings.py`)

```python
# Depth and Crawling Configuration
MAX_DEPTH = 3                    # Maximum crawling depth
DEPTH_PRIORITY = 1               # Priority adjustment for deeper pages
SCHEDULER_DISK_QUEUE = 'scrapy.squeues.PickleLifoDiskQueue'
SCHEDULER_MEMORY_QUEUE = 'scrapy.squeues.LifoMemoryQueue'

# Request Configuration
ROBOTSTXT_OBEY = True           # Respect robots.txt
DOWNLOAD_DELAY = 1              # Base delay between requests
RANDOMIZE_DOWNLOAD_DELAY = True # Randomize delays
DOWNLOAD_TIMEOUT = 30           # Request timeout in seconds

# User Agent Configuration
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36',
    # ... more user agents
]

# Rate Limiting Configuration
RATE_LIMIT_ENABLED = True
RATE_LIMIT_MIN_DELAY = 1.0      # Minimum delay between requests
RATE_LIMIT_MAX_DELAY = 10.0     # Maximum delay between requests
RATE_LIMIT_REQUESTS_PER_MINUTE = 30

# Retry Configuration
RETRY_ENABLED = True
RETRY_TIMES = 3                 # Maximum retry attempts
RETRY_HTTP_CODES = [500, 502, 503, 504, 408, 429]

# Language and Quality Configuration
SUPPORTED_LANGUAGES = ['es', 'en', 'fr', 'de', 'it', 'pt']
MIN_CONTENT_LENGTH = 100        # Minimum content length to process
MIN_EMAIL_QUALITY_SCORE = 0.5   # Minimum email quality score (0-1)

# Database and Logging Configuration
LOG_LEVEL = 'INFO'
LOG_TO_DATABASE = True
STATS_CLASS = 'scrapy.statscollectors.MemoryStatsCollector'
```

### API Configuration (`backend/app/core/config.py`)

```python
class ScraperConfig:
    # Scraping Parameters
    max_concurrent_spiders = 1
    spider_timeout = 3600  # 1 hour timeout
    max_memory_usage = 512  # MB

    # Database Settings
    database_url = "sqlite:///./leads.db"
    echo_sql = False  # Set to True for debugging

    # API Settings
    api_host = "127.0.0.1"
    api_port = 8000
    api_workers = 1
```

## API Endpoints

### Job Management

#### Create Job
```http
POST /api/jobs
Content-Type: application/json

{
    "start_url": "https://example.com",
    "config": {
        "max_depth": 3,
        "languages": ["es", "en"],
        "delay": 2.0,
        "user_agent_rotation": true,
        "rate_limiting": true,
        "respect_robots": true
    }
}
```

**Response:**
```json
{
    "job_id": "uuid-1234-abcd",
    "status": "starting",
    "message": "Advanced scraping job started.",
    "config": {
        "max_depth": 3,
        "languages": ["es", "en"],
        "delay": 2.0,
        "user_agent_rotation": true,
        "rate_limiting": true,
        "respect_robots": true
    }
}
```

#### Get Job Status
```http
GET /api/jobs/{job_id}
```

**Response:**
```json
{
    "job_id": "uuid-1234-abcd",
    "status": "running",
    "start_time": "2023-10-27T10:00:00Z",
    "end_time": null,
    "config": { ... },
    "stats": {
        "processed_urls": 150,
        "queue_size": 45,
        "leads_found": 23,
        "emails_found": 45,
        "errors_count": 2,
        "avg_response_time": 1.2,
        "current_depth": 2
    },
    "performance": {
        "requests_per_minute": 25.3,
        "memory_usage_mb": 89,
        "cpu_usage_percent": 15.2
    }
}
```

#### Stop Job
```http
DELETE /api/jobs/{job_id}
```

### Statistics and Monitoring

#### Get System Stats
```http
GET /api/stats
```

**Response:**
```json
{
    "active_jobs": 1,
    "total_jobs_today": 5,
    "total_leads_today": 127,
    "total_emails_today": 234,
    "system_health": {
        "database_status": "healthy",
        "memory_usage_mb": 245,
        "cpu_usage_percent": 12.5,
        "disk_usage_gb": 2.1
    },
    "recent_activity": [
        {
            "timestamp": "2023-10-27T10:30:00Z",
            "job_id": "uuid-1234-abcd",
            "action": "url_processed",
            "details": "Processed https://example.com/page1"
        }
    ]
}
```

## Database Schema

### New Tables for Advanced Features

#### `scraping_sessions`
Tracks scraping sessions with advanced monitoring.

```sql
CREATE TABLE scraping_sessions (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    session_id TEXT UNIQUE NOT NULL,
    job_id TEXT NOT NULL,
    start_url TEXT NOT NULL,
    config TEXT NOT NULL,  -- JSON configuration
    status TEXT NOT NULL,  -- 'running', 'completed', 'failed', 'stopped'
    start_time DATETIME NOT NULL,
    end_time DATETIME,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
);
```

#### `scraping_logs`
Detailed logging of all scraping activities.

```sql
CREATE TABLE scraping_logs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    session_id TEXT NOT NULL,
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
    level TEXT NOT NULL,  -- 'DEBUG', 'INFO', 'WARNING', 'ERROR'
    category TEXT NOT NULL,  -- 'request', 'parsing', 'error', 'performance'
    message TEXT NOT NULL,
    url TEXT,
    metadata TEXT,  -- JSON additional data
    FOREIGN KEY (session_id) REFERENCES scraping_sessions(session_id)
);
```

#### Enhanced `websites` table
```sql
ALTER TABLE websites ADD COLUMN quality_score REAL DEFAULT 0.0;
ALTER TABLE websites ADD COLUMN language_confidence REAL DEFAULT 0.0;
ALTER TABLE websites ADD COLUMN last_crawled DATETIME;
ALTER TABLE websites ADD COLUMN crawl_count INTEGER DEFAULT 1;
ALTER TABLE websites ADD COLUMN session_id TEXT;
```

#### Enhanced `emails` table
```sql
ALTER TABLE emails ADD COLUMN quality_score REAL DEFAULT 0.0;
ALTER TABLE emails ADD COLUMN validation_status TEXT DEFAULT 'pending';
ALTER TABLE emails ADD COLUMN source_context TEXT;
ALTER TABLE emails ADD COLUMN session_id TEXT;
```

## Usage Examples

### Basic Advanced Scraping Job

```python
import requests

# Start an advanced scraping job
response = requests.post('http://localhost:8000/api/jobs', json={
    "start_url": "https://example.com",
    "config": {
        "max_depth": 2,
        "languages": ["es", "en"],
        "delay": 1.5,
        "user_agent_rotation": True,
        "rate_limiting": True,
        "respect_robots": True
    }
})

job_data = response.json()
job_id = job_data['job_id']

# Monitor progress
while True:
    status_response = requests.get(f'http://localhost:8000/api/jobs/{job_id}')
    status = status_response.json()

    print(f"Status: {status['status']}")
    print(f"Processed URLs: {status['stats']['processed_urls']}")
    print(f"Leads found: {status['stats']['leads_found']}")

    if status['status'] in ['completed', 'failed', 'stopped']:
        break

    time.sleep(5)
```

### Advanced Configuration Example

```python
# High-volume scraping configuration
advanced_config = {
    "start_url": "https://large-site.com",
    "config": {
        "max_depth": 5,
        "languages": ["es", "en", "fr"],
        "delay": 3.0,
        "user_agent_rotation": True,
        "rate_limiting": True,
        "respect_robots": True,
        "max_concurrent_requests": 16,
        "download_timeout": 60,
        "retry_times": 5,
        "min_content_length": 200,
        "min_email_quality_score": 0.7
    }
}
```

## Monitoring and Troubleshooting

### Log Analysis

The system provides detailed logging through the database. Common log categories:

- **request**: HTTP request/response details
- **parsing**: Content extraction and processing
- **error**: Errors and exceptions with context
- **performance**: Performance metrics and bottlenecks

### Common Issues and Solutions

#### High Error Rates
- **Cause**: Anti-bot detection, network issues, or server overload
- **Solution**: Increase delays, enable user-agent rotation, reduce concurrent requests

#### Low Quality Leads
- **Cause**: Poor content filtering or low-quality sources
- **Solution**: Adjust quality thresholds, add language filters, improve seed URLs

#### Memory Issues
- **Cause**: Large crawling sessions or memory leaks
- **Solution**: Reduce concurrent requests, implement session limits, monitor memory usage

#### Rate Limiting Problems
- **Cause**: Too aggressive scraping or server restrictions
- **Solution**: Increase delays, implement exponential backoff, respect robots.txt

### Performance Tuning

#### For Speed
- Reduce `DOWNLOAD_DELAY`
- Increase `CONCURRENT_REQUESTS`
- Disable `ROBOTSTXT_OBEY` (if legal)
- Use faster DNS resolution

#### For Reliability
- Increase `DOWNLOAD_TIMEOUT`
- Enable `RETRY_ENABLED`
- Use multiple user agents
- Implement rate limiting

#### For Quality
- Increase `MIN_CONTENT_LENGTH`
- Set higher `MIN_EMAIL_QUALITY_SCORE`
- Enable language filtering
- Use advanced duplicate detection

## Security Considerations

### Anti-Detection Measures
- User-agent rotation prevents fingerprinting
- Rate limiting avoids triggering anti-bot systems
- Random delays simulate human behavior
- Request fingerprinting prevents repetitive patterns

### Data Privacy
- Respect robots.txt and website terms of service
- Implement proper data retention policies
- Use HTTPS for API communications
- Encrypt sensitive configuration data

### System Security
- Input validation on all API endpoints
- SQL injection prevention through ORM
- XSS protection in web interfaces
- Secure logging without exposing sensitive data

## Deployment and Scaling

### Single Machine Deployment
```bash
# Install dependencies
pip install -r requirements.txt

# Initialize database
python -c "from backend.app.database.database import init_db; init_db()"

# Start the backend
python backend/run_backend.py
```

### Performance Optimization
- Use SSD storage for database
- Configure appropriate memory limits
- Monitor system resources
- Implement log rotation

### Scaling Considerations
- Database connection pooling
- Load balancing for multiple instances
- Distributed queue management
- Monitoring and alerting systems

## Future Enhancements

### Planned Features
- **Proxy rotation**: Support for proxy servers
- **CAPTCHA handling**: Automatic CAPTCHA solving
- **JavaScript rendering**: Support for dynamic content
- **Cloud deployment**: Docker and Kubernetes support
- **Advanced analytics**: Machine learning for lead scoring

> ‚ö†Ô∏è **Autorizaci√≥n de Machine Learning Requerida**: La funcionalidad de machine learning para lead scoring requiere autorizaci√≥n expl√≠cita del usuario para su implementaci√≥n. No est√° incluida en el scope actual del proyecto y se implementar√° solo si el usuario lo solicita espec√≠ficamente.

### Integration Points
- **CRM integration**: Direct export to CRM systems
- **Email validation**: Third-party email verification
- **Social media scraping**: LinkedIn, Twitter integration
- **API marketplace**: White-label API access

---

This documentation covers all advanced scraper features implemented in Phase 2. For questions or support, refer to the main technical plan or contact the development team.