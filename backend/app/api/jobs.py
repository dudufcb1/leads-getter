"""
Endpoints para gesti칩n de trabajos de scraping.
"""

import uuid
from typing import Dict, Any
from fastapi import APIRouter, HTTPException, BackgroundTasks, Depends
from pydantic import BaseModel, Field
from sqlalchemy.orm import Session
from ..database.database import get_db
from ..database.models import ScrapingQueue
from ..scraper.run_scraper import run_scraper

router = APIRouter()


class JobConfig(BaseModel):
    """Configuraci칩n para un trabajo de scraping."""
    start_url: str = Field(..., description="URL inicial para el scraping")
    depth: int = Field(3, description="Profundidad m치xima de scraping", ge=1, le=10)
    languages: list[str] = Field(["es", "en"], description="Idiomas a buscar")
    delay: float = Field(2.0, description="Delay entre requests", ge=0.1, le=10.0)


class JobResponse(BaseModel):
    """Respuesta para creaci칩n de trabajo."""
    job_id: str
    status: str
    message: str


class JobStatus(BaseModel):
    """Estado de un trabajo de scraping."""
    job_id: str
    status: str
    start_time: str
    stats: Dict[str, Any]


@router.post("/", response_model=JobResponse)
async def create_job(
    config: JobConfig,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db)
):
    """
    Inicia un nuevo trabajo de scraping.

    - **start_url**: URL inicial para comenzar el scraping
    - **depth**: Profundidad m치xima de scraping (1-10)
    - **languages**: Lista de idiomas a buscar
    - **delay**: Delay entre requests en segundos
    """
    try:
        # Generar ID 칰nico para el trabajo
        job_id = str(uuid.uuid4())

        # Verificar si la URL ya existe en la cola
        existing_queue_item = db.query(ScrapingQueue).filter_by(url=config.start_url).first()
        
        if existing_queue_item:
            # Si ya existe, actualizar el estado si es necesario
            if existing_queue_item.status in ["failed", "completed"]:
                existing_queue_item.status = "pending"
                existing_queue_item.attempts = 0
                db.commit()
                queue_item = existing_queue_item
            else:
                # Si ya est치 pending o processing, usar el existente
                queue_item = existing_queue_item
        else:
            # Crear nueva entrada en la cola de scraping
            queue_item = ScrapingQueue(
                url=config.start_url,
                priority=0,
                depth_level=0,
                status="pending",
                attempts=0
            )
            db.add(queue_item)
            db.commit()
            db.refresh(queue_item)

        # Iniciar el scraper en background
        background_tasks.add_task(run_scraper, job_id, config.start_url, config.depth)

        return JobResponse(
            job_id=job_id,
            status="starting",
            message="Scraping job started successfully."
        )

    except Exception as e:
        db.rollback()
        import traceback
        error_detail = f"Error creating job: {str(e)}\nTraceback: {traceback.format_exc()}"
        print(f"游뚿 ERROR IN CREATE_JOB: {error_detail}")
        raise HTTPException(status_code=500, detail=error_detail)


@router.get("/{job_id}", response_model=JobStatus)
async def get_job_status(job_id: str, db: Session = Depends(get_db)):
    """
    Obtiene el estado de un trabajo espec칤fico.

    - **job_id**: ID del trabajo de scraping
    """
    # Por ahora, devolver un estado b치sico
    # En una implementaci칩n completa, esto consultar칤a la base de datos
    # para obtener estad칤sticas reales del trabajo
    return JobStatus(
        job_id=job_id,
        status="running",
        start_time="2023-10-27T10:00:00Z",
        stats={
            "processed_urls": 0,
            "queue_size": 1,
            "leads_found": 0
        }
    )


@router.delete("/{job_id}", response_model=JobResponse)
async def stop_job(job_id: str):
    """
    Detiene (cancela) un trabajo en ejecuci칩n.

    - **job_id**: ID del trabajo a detener
    """
    # Por ahora, devolver una respuesta b치sica
    # En una implementaci칩n completa, esto deber칤a:
    # 1. Verificar que el trabajo existe
    # 2. Detener el proceso de scraping si est치 corriendo
    # 3. Actualizar el estado en la base de datos
    return JobResponse(
        job_id=job_id,
        status="stopping",
        message="Scraping job is being stopped."
    )