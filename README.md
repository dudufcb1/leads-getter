# Leads Generator - Advanced Scraper

Un sistema avanzado de generaciÃ³n de leads mediante web scraping con capacidades de scraping encadenado, detecciÃ³n inteligente de emails, y monitoreo completo.

## ğŸš€ CaracterÃ­sticas Principales

### Scraping Avanzado
- **Scraping encadenado** con control de profundidad configurable
- **DetecciÃ³n avanzada de emails** con mÃºltiples patrones y validaciÃ³n
- **RotaciÃ³n de user-agents** para evitar bloqueos
- **Rate limiting adaptativo** por dominio
- **Manejo robusto de errores** con reintentos y backoff exponencial

### Calidad de Datos
- **Filtrado por idioma** con detecciÃ³n automÃ¡tica
- **DeduplicaciÃ³n inteligente** de contenido
- **PuntuaciÃ³n de calidad** para leads y emails
- **ValidaciÃ³n de formato** de emails

### Monitoreo y Control
- **Logging completo** en base de datos
- **EstadÃ­sticas en tiempo real** del proceso de scraping
- **API REST** para control remoto
- **Sesiones de scraping** con seguimiento detallado

## ğŸ“‹ Requisitos del Sistema

- **Python**: 3.8 o superior
- **Sistema Operativo**: Linux (desarrollo y testing)
- **Memoria**: MÃ­nimo 512MB RAM
- **Espacio en disco**: 100MB para base de datos y logs

### Dependencias del Sistema (Linux)

```bash
# Ubuntu/Debian
sudo apt-get update
sudo apt-get install python3-tk python3-dev build-essential

# Fedora/CentOS
sudo dnf install python3-tkinter python3-devel gcc

# Arch Linux
sudo pacman -S tk python
```

## ğŸ› ï¸ InstalaciÃ³n

1. **Clonar el repositorio**
   ```bash
   git clone <repository-url>
   cd leads-generator
   ```

2. **Crear entorno virtual**
   ```bash
   python3 -m venv .venv
   source .venv/bin/activate  # Linux/macOS
   # o
   .venv\Scripts\activate     # Windows
   ```

3. **Instalar dependencias**
   ```bash
   pip install -r backend/requirements.txt
   ```

4. **Inicializar base de datos**
   ```bash
   cd backend
   python -c "from app.database.database import init_db; init_db()"
   ```

## ğŸš€ Uso BÃ¡sico

### Iniciar el Servidor Backend

```bash
cd backend
python run_backend.py
```

El servidor estarÃ¡ disponible en `http://127.0.0.1:8000`

### Ejemplo de Uso de la API

```python
import requests

# Iniciar trabajo de scraping avanzado
response = requests.post('http://localhost:8000/api/jobs', json={
    "start_url": "https://example.com",
    "config": {
        "max_depth": 3,
        "languages": ["es", "en"],
        "delay": 2.0,
        "user_agent_rotation": True,
        "rate_limiting": True
    }
})

job_id = response.json()['job_id']

# Monitorear progreso
status = requests.get(f'http://localhost:8000/api/jobs/{job_id}').json()
print(f"Estado: {status['status']}")
print(f"URLs procesadas: {status['stats']['processed_urls']}")
```

## âš™ï¸ ConfiguraciÃ³n Avanzada

### ParÃ¡metros de Scraping

| ParÃ¡metro | DescripciÃ³n | Valor por Defecto |
|-----------|-------------|-------------------|
| `MAX_DEPTH` | Profundidad mÃ¡xima de crawling | 3 |
| `DOWNLOAD_DELAY` | Retraso entre requests (segundos) | 1.0 |
| `CONCURRENT_REQUESTS` | Requests concurrentes | 16 |
| `DOWNLOAD_TIMEOUT` | Timeout de requests (segundos) | 30 |
| `RETRY_TIMES` | NÃºmero mÃ¡ximo de reintentos | 3 |

### ConfiguraciÃ³n de Calidad

```python
# En backend/app/scraper/settings.py
SUPPORTED_LANGUAGES = ['es', 'en', 'fr', 'de']
MIN_CONTENT_LENGTH = 100
MIN_EMAIL_QUALITY_SCORE = 0.5
```

## ğŸ“Š Monitoreo

### EstadÃ­sticas en Tiempo Real

```bash
# Obtener estadÃ­sticas del sistema
curl http://localhost:8000/api/stats
```

### Logs de Base de Datos

Los logs se almacenan en la tabla `scraping_logs` con las siguientes categorÃ­as:
- `request`: Detalles de requests HTTP
- `parsing`: ExtracciÃ³n y procesamiento de contenido
- `error`: Errores y excepciones
- `performance`: MÃ©tricas de rendimiento

## ğŸ”§ SoluciÃ³n de Problemas

### Problemas Comunes

#### Error de Memoria
```bash
# Reducir requests concurrentes
export CONCURRENT_REQUESTS=8
```

#### Bloqueos por Anti-Bot
```python
# Aumentar delays y habilitar rotaciÃ³n
config = {
    "delay": 3.0,
    "user_agent_rotation": True,
    "rate_limiting": True
}
```

#### Emails de Baja Calidad
```python
# Ajustar filtros de calidad
config = {
    "min_email_quality_score": 0.7,
    "languages": ["es"],
    "min_content_length": 200
}
```

### Debugging

```bash
# Ejecutar con logging detallado
export LOG_LEVEL=DEBUG
python run_backend.py
```

## ğŸ“š DocumentaciÃ³n

- **[Plan TÃ©cnico](docs/technical_plan.md)**: Arquitectura general del sistema
- **[Scraper Avanzado](docs/advanced_scraper.md)**: DocumentaciÃ³n completa de caracterÃ­sticas avanzadas
- **[API Reference](docs/api_reference.md)**: Referencia completa de la API REST

## ğŸ—ï¸ Arquitectura

```
leads-generator/
â”œâ”€â”€ backend/                 # Servidor y lÃ³gica de negocio
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/            # Endpoints REST
â”‚   â”‚   â”œâ”€â”€ scraper/        # Motor de Scrapy
â”‚   â”‚   â”œâ”€â”€ database/       # Modelos y conexiÃ³n DB
â”‚   â”‚   â””â”€â”€ core/           # ConfiguraciÃ³n
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ docs/                   # DocumentaciÃ³n
â””â”€â”€ README.md
```

## ğŸ¤ ContribuciÃ³n

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## ğŸ“ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver el archivo `LICENSE` para mÃ¡s detalles.

## ğŸ†˜ Soporte

Para soporte tÃ©cnico o preguntas:
- Revisar la [documentaciÃ³n avanzada](docs/advanced_scraper.md)
- Consultar los [logs de troubleshooting](docs/troubleshooting.md)
- Abrir un issue en el repositorio

---

**Nota**: Este sistema estÃ¡ diseÃ±ado para uso Ã©tico y respetuoso con los tÃ©rminos de servicio de los sitios web. AsegÃºrate de cumplir con las leyes locales y las polÃ­ticas de los sitios que visites.